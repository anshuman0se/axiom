\documentclass{book}
%\newcommand{\VolumeName}{Volume 2: Axiom Users Guide}
%\input{bookheader.tex}
\pagenumbering{arabic}
\mainmatter
\setcounter{chapter}{0} % Chapter 1

\usepackage{makeidx}
\makeindex
\begin{document}
\begin{verbatim}
\start
Date: Mon, 02 Jan 2012 16:55:04 -0500
From: Camm Maguire
To: Tim Daly
Subject: Re: read-from-string bug

Thanks!  Will post on this shortly.  In the mean time, I have the
following ia64 only axiom build bug:

(sid)camm@merulo:~/axiom-20110301a/int/algebra$  ../../obj/linux/bin/interpsys
GCL (GNU Common Lisp)  2.6.7 CLtL1    May 12 2011 03:42:00
Source License: LGPL(gcl,gmp), GPL(unexec,bfd,xgcl)
Binary License:  GPL due to GPL'ed components: (XGCL READLINE UNEXEC)
Modifications of this banner must retain notice of a compatible license
Dedicated to the memory of W. Schelter

Use (help) to get some basic information on how to use GCL.
Temporary directory for compiler files set to /tmp/
                        AXIOM Computer Algebra System 
                         Version: Axiom (March 2011)
               Timestamp: Monday January 2, 2012 at 21:29:17 
-----------------------------------------------------------------------------
   Issue )copyright to view copyright notices.
   Issue )summary for a summary of useful system commands.
   Issue )quit to leave AXIOM and return to shell.
   Visit http://axiom-developer.org for more information
-----------------------------------------------------------------------------
 
(1) -> )co AHYP
   Compiling AXIOM source code from file 
      /home/camm/axiom-20110301a/int/algebra/AHYP.spad using old system
      compiler.
   AHYP abbreviates category ArcHyperbolicFunctionCategory 
(1) -> 

should be

camm@localhost:~/debian/axiom/axiom-20110301a/int/algebra$ ../../obj/linux/bin/interpsys
GCL (GNU Common Lisp)  2.6.7 CLtL1    May 11 2011 20:18:12
Source License: LGPL(gcl,gmp), GPL(unexec,bfd,xgcl)
Binary License:  GPL due to GPL'ed components: (XGCL READLINE UNEXEC)
Modifications of this banner must retain notice of a compatible license
Dedicated to the memory of W. Schelter

Use (help) to get some basic information on how to use GCL.
Temporary directory for compiler files set to /tmp/
                        AXIOM Computer Algebra System 
                         Version: Axiom (March 2011)
               Timestamp: Thursday July 14, 2011 at 19:25:29 
-----------------------------------------------------------------------------
   Issue )copyright to view copyright notices.
   Issue )summary for a summary of useful system commands.
   Issue )quit to leave AXIOM and return to shell.
   Visit http://axiom-developer.org for more information
-----------------------------------------------------------------------------
 
   Re-reading compress.daase   Re-reading interp.daase
   Re-reading operation.daase
   Re-reading category.daase
   Re-reading browse.daase
(1) -> )co AHYP
   Compiling AXIOM source code from file 
      /home/camm/debian/axiom/axiom-20110301a/int/algebra/AHYP.spad 
      using old system compiler.
   AHYP abbreviates category ArcHyperbolicFunctionCategory 
------------------------------------------------------------------------
   initializing nrlib AHYP for ArcHyperbolicFunctionCategory 
   compiling into nrlib AHYP 

;;;     ***       |ArcHyperbolicFunctionCategory| REDEFINED
Time: 0 SEC.

   finalizing nrlib AHYP 
   Processing ArcHyperbolicFunctionCategory for Browser database:
--------(acosh ($ $))---------
--------(acoth ($ $))---------
--------(acsch ($ $))---------
--------(asech ($ $))---------
--------(asinh ($ $))---------
--------(atanh ($ $))---------
--------constructor---------
; (DEFUN |ArcHyperbolicFunctionCategory| ...) is being compiled.
;; The variable |ArcHyperbolicFunctionCategory;AL| is undefined.
;; The compiler will assume this variable is a global.
------------------------------------------------------------------------
   ArcHyperbolicFunctionCategory is now explicitly exposed in frame 
      initial 
   ArcHyperbolicFunctionCategory will be automatically loaded when 
      needed from 
      /home/camm/debian/axiom/axiom-20110301a/int/algebra/AHYP.nrlib/code



Where do I look?

Take care,

Tim Daly writes:

> gcl-2.6.8 built from repository source on Dec 22, 2011
>
> This works:
>
> (read-from-string ";;;" nil nil :start 0)
>
> This fails:
>
> (read-from-string ";;;" nil nil :start 0 :preserve-whitespace t)
>
> The read-from-string calls read-preserving-whitespace 
> which wanders off to lsp/gcl_iolib.c
> which is where you lost me.

\start
Date: Mon, 02 Jan 2012 16:41:56 -0500
From: Camm Maguire
To: Bill Page
Subject: Re: Overflow
Cc: Udo Ott

Greetings!  Using GCL, you can expand the stacks at runtime using (setq
si::*multiply-stacks* n), where n is typically some small positive power
of 2.

Take care,

Bill Page writes:

> Udo Ott,
>
> As you might be aware, the message:
>
>   frame stack overflow
>
> is generated by the Lisp system apon which the version of Axiom that
> you are using is built. Exactly how to configure the Lisp memory
> configuration to allow a greater depth of recursion or other possible
> constraints is highly dependent on the particular version of Lisp you
> are using. I expect that Axiom-Ubuntu-64Bit is using GCL but I am not
> certain. If so you might look online for configuration suggestions
> from GCL users.  Axiom and especially it's variants OpenAxiom and
> FriCAS can be built with several different Lisp systems and each
> manages memory a little differently so it is possible that you might
> get more "mileage" with Axiom (or OpenAxiom or FriCAS) built with some
> other Lisp such as SBCL, CLISP, or ECL.  Again, exactly how to do this
> depends on the particular variant of Axiom and Lisp.
>
> You may be able to find some hints about this at
> http://axiom-wiki.newsynthesis.org For example the page
>
>   http://axiom-wiki.newsynthesis.org/SandBoxAldorGenerator
>
> but like most wiki's this information is only as good as the
> dedication of the people who created and modified the page.
>
> AXIOM-XL is an obsolete name for Aldor which many years ago was
> intended to become the "next generation" compiler for Axiom. At this
> time only the FriCAS project supports the use of Aldor. Due to
> licensing constraints Aldor must be compiled separately from FriCAS.
> See http://www.aldor.org  or
> http://algebraist.origo.ethz.ch/wiki/algebraist.  When used with
> FriCAS the code generated by Aldor is compiled using the same
> underlying Lisp system, so memory constraints are similar to using
> SPAD.  But Aldor can also be used in a "stand alone" mode in which it
> compiles and links to a C runtime system.  In this case the memory
> usage is likely to be quite different (although not necessary better)
> than when using Lisp.
>
> I hope some of this information helps.
>
> Regards,
> Bill Page.
>
>
> On Sun, Dec 18, 2011 at 7:29 AM, Udo Ott Udo Ott wrote:
>> Hello,
>>
>> since some days I am using Axiom-Ubuntu-64Bit in order to construct
>> combinatorial structures with the help of a back-tracking algorithm.
>>
>> Unfortunately, after about 2**12 calls the function terminates with the
>> system error
>>
>> frame stack overflow
>>
>> What can I do to allow more calls?
>>
>> Secend question: Is there a version of Axiom-XL for Ubuntu 64-Bit?

\start
Date: Mon, 02 Jan 2012 16:40:16 -0500
From: Camm Maguire
To: Leon Dutoit
Subject: Re: axiom and glibc-2.14

Greetings!  Can you post a failing build log please?

Take care,

L=C3=A9on Dutoit Leon Dutoit writes:

> Compilation of axiom fails on slackware64 (current) because in glibc-2.14=
 support of rpc has been suppressed.
>
> The problem is with gcl-2.6.8pre4 (and maybe after).
>
> To have rpc support the library "libtiprc" must be installed, but then at=
 the link stage "-ltirpc" should be added.
> This is apparently not checked in the configure script in gcl-2.6.8pre4.

\start
Date: Tue, 3 Jan 2012 07:14:28 +0000 (GMT)
From: Leon Dutoit
To: Camm Maguire
Subject: Re :  axiom and glibc-2.14

 
Greetings!=A0 Can you post a failing build lo=
g please?

Take care,

L=E9on Dutoit Leon Dutoit writes:
=

> Compilation of axiom fails on slackware64 (current) because in
  glibc-2.14 support of rpc has been suppressed.

> The problem is with gcl-2.6.8pre4 (and maybe after).

> To have rpc support the library "libtiprc" must be installed, but
> then at the link stage "-ltirpc" should be added.  This is
> apparently not checked in the configure script in gcl-2.6.8pre4.

I get this :


touch raw_pre_gcl_map
gcc -o raw_pre_=
gcl /d1/axiom/obj/slackware/lib/cfuns-c.o /d1/axiom/obj/slackware/lib/sockio-c.o \
=A0=A0=A0=A0=A0=A0=A0 -L.=A0 -Wl,-Map raw_pre_gcl_map=A0=A0 -lpre_gcl -lm=A0 -lc -lgclp /d1/axiom/obj/slackware/lib/libspad.a 
./libpre_gc=
l.a(funlink.o): In function `siGxdr_open':
funlink.c:(.text+0x112): undef=
ined reference to `xdrstdio_create'
./libpre_gcl.a(funlink.o): In functio=
n `siGxdr_read':
funlink.c:(.text+0x1e6): undefined reference to `xdr_double'
funlink.c:(.text+0x206): undefined reference to `xdr_float'
funlin=
k.c:(.text+0x22e): undefined reference to `xdr_long'
funlink.c:(.text+0x2=
7a): undefined reference to `xdr_short'
funlink.c:(.text+0x282): undefined reference to `xdr_float'
funlink.c:(.text+0x2a6): undefined reference to `xdr_array'
funlink.c:(.text+0x2c2): undefined reference to `xdr_double'
funlink.c:(.text+0x2d2): undefined reference to `xdr_long'
./libpre_gcl.a(funlink.o): In function `siGxdr_write':
funlink.c:(.text+0x385): undefined reference to `xdr_double'
funlink.c:(.text+0x3a5): undefined reference to `xdr_float'
funlink.c:(.text+0x3cd): undefined reference to `xdr_long'
funlink.c:(.text+0x402): undefined reference to `xdr_short'
funlink.c:(.text+0x412): undefined reference to `xdr_float'
funlink.c:(.text+0x436): undefined reference to `xdr_array'
funlink.c:(.text+0x452): undefined reference to `xdr_long'
funlink.c:(.text+0x462): undefined reference to `xdr_double'
collect2: ld returned 1 exit status
make[4]: *** [raw_pre_gcl_map] Error 1
make[4]: Leaving directory `/d1/axiom/lsp/gcl-2.6.8pre4/unixport'
make[3]: *** [unixport/saved_pre_gcl] Error 2
make[3]: Leaving directory `/d1/axiom/lsp/gcl-2.6.8pre4'
/bin/sh: line 5: unixport/saved_gcl: No such file or directory
make[2]: *** [gcldir] Error 127
make[2]: Leaving directory `/d1/axiom/lsp'
make[1]: *** [lspdir] Error 2
make[1]: Leaving directory `/d1/axiom'
make: *** [all] Error 2

\start
Date: Tue, 03 Jan 2012 10:47:56 -0500
From: Camm Maguire
To: Tim Daly
Subject: Re: read-from-string bug

Greetings!  This looks right -- please try and let me know and I'll
commit.

Index: read.d
===================================================================
RCS file: /sources/gcl/gcl/o/read.d,v
retrieving revision 1.14.4.1.2.2.2.4.6.1.6.1.4.9
diff -u -u -r1.14.4.1.2.2.2.4.6.1.6.1.4.9 read.d
--- read.d	27 Aug 2010 16:53:25 -0000	1.14.4.1.2.2.2.4.6.1.6.1.4.9
+++ read.d	3 Jan 2012 15:46:22 -0000
@@ -2057,17 +2057,23 @@
 			goto READ;
 		}
 	}
-	if (eof_errorp == Cnil && recursivep == Cnil)
-		@(return eof_value)
-	end_of_stream(strm);
+	/* if (eof_errorp == Cnil && recursivep == Cnil) */
+	/* 	@(return eof_value) */
+	/* end_of_stream(strm); */
 
 READ:
 	if (recursivep == Cnil)
 		preserving_whitespace_flag = TRUE;
-	if (recursivep == Cnil)
+        detect_eos_flag = TRUE;
+        if (recursivep == Cnil)
 		x = read_object_non_recursive(strm);
 	else
 		x = read_object_recursive(strm);
+	if (x == OBJNULL) {
+		if (eof_errorp == Cnil && recursivep == Cnil)
+			@(return eof_value)
+		end_of_stream(strm);
+	}
 	@(return x)
 @)
 


Tim Daly writes:

> gcl-2.6.8 built from repository source on Dec 22, 2011
>
> This works:
>
> (read-from-string ";;;" nil nil :start 0)
>
> This fails:
>
> (read-from-string ";;;" nil nil :start 0 :preserve-whitespace t)
>
> The read-from-string calls read-preserving-whitespace 
> which wanders off to lsp/gcl_iolib.c
> which is where you lost me.

\start
Date: Tue, 03 Jan 2012 11:39:06 -0500
From: Camm Maguire
To: Leon Dutoit
Subject: Re: Re :  axiom and glibc-2.14

Greetings!  Can you try the following to see if it works for you?

Index: configure.in
===================================================================
RCS file: /sources/gcl/gcl/configure.in,v
retrieving revision 1.112.4.1.2.2.2.47.2.3.2.1.4.2.4.2.4.64
diff -u -u -r1.112.4.1.2.2.2.47.2.3.2.1.4.2.4.2.4.64 configure.in
--- configure.in	20 Jun 2011 15:41:52 -0000	1.112.4.1.2.2.2.47.2.3.2.1.4.2.4.2.4.64
+++ configure.in	3 Jan 2012 16:36:41 -0000
@@ -1038,6 +1038,11 @@
 	AC_SUBST(BUILD_BFD)
 fi

+
+AC_CHECK_FUNC(xdr_double,,
+	   AC_CHECK_LIB(tirpc,xdr_double,TLIBS="$TLIBS -ltirpc",AC_MSG_ERROR([Need xdr_double])))
+
+
 #AC_CONFIG_SUBDIRS($MY_SUBDIRS)

 # Find where Data begins.  This is used by the storage allocation
Index: configure
==========================
==========================
================RCS file: /sources/gcl/gcl/configure,v
retrieving revision 1.107.4.1.2.2.2.48.2.3.2.1.4.2.4.2.4.64
diff -u -u -r1.107.4.1.2.2.2.48.2.3.2.1.4.2.4.2.4.64 configure
--- configure	20 Jun 2011 15:41:48 -0000	1.107.4.1.2.2.2.48.2.3.2.1.4.2.4.2.4.64
+++ configure	3 Jan 2012 16:36:46 -0000
@@ -1801,6 +1801,73 @@

 } # ac_fn_c_check_member

+# ac_fn_c_check_func LINENO FUNC VAR
+# ----------------------------------
+# Tests whether FUNC exists, setting the cache variable VAR accordingly
+ac_fn_c_check_func ()
+{
+  as_lineno=${as_lineno-"$1"} as_lineno_stack=as_lineno_stack=$as_lineno_stack
+  { $as_echo "$as_me:${as_lineno-$LINENO}: checking for $2" >&5
+$as_echo_n "checking for $2... " >&6; }
+if eval \${$3+:} false; then :
+  $as_echo_n "(cached) " >&6
+else
+  cat confdefs.h - <<_ACEOF >conftest.$ac_ext
+/* end confdefs.h.  */
+/* Define $2 to an innocuous variant, in case <limits.h> declares $2.
+   For example, HP-UX 11i <limits.h> declares gettimeofday.  */
+#define $2 innocuous_$2
+
+/* System header to define __stub macros and hopefully few prototypes,
+    which can conflict with char $2 (); below.
+    Prefer <limits.h> to <assert.h> if __STDC__ is defined, since
+    <limits.h> exists even on freestanding compilers.  */
+
+#ifdef __STDC__
+# include <limits.h>
+#else
+# include <assert.h>
+#endif
+
+#undef $2
+
+/* Override any GCC internal prototype to avoid an error.
+   Use char because int might match the return type of a GCC
+   builtin and then its argument prototype would still apply.  */
+#ifdef __cplusplus
+extern "C"
+#endif
+char $2 ();
+/* The GNU C library defines this for functions which it implements
+    to always fail with ENOSYS.  Some functions are actually named
+    something starting with __ and the normal name is an alias.  */
+#if defined __stub_$2 || defined __stub___$2
+choke me
+#endif
+
+int
+main ()
+{
+return $2 ();
+  ;
+  return 0;
+}
+_ACEOF
+if ac_fn_c_try_link "$LINENO"; then :
+  eval "$3=yes"
+else
+  eval "$3=no"
+fi
+rm -f core conftest.err conftest.$ac_objext \
+    conftest$ac_exeext conftest.$ac_ext
+fi
+eval ac_res=\$$3
+	       { $as_echo "$as_me:${as_lineno-$LINENO}: result: $ac_res" >&5
+$as_echo "$ac_res" >&6; }
+  eval $as_lineno_stack; ${as_lineno_stack:+:} unset as_lineno
+
+} # ac_fn_c_check_func
+
 # ac_fn_c_compute_int LINENO EXPR VAR INCLUDES
 # --------------------------------------------
 # Tries to find the compile-time value of EXPR in a program that includes
@@ -1978,73 +2045,6 @@
   as_fn_set_status $ac_retval

 } # ac_fn_c_compute_int
-
-# ac_fn_c_check_func LINENO FUNC VAR
-# ----------------------------------
-# Tests whether FUNC exists, setting the cache variable VAR accordingly
-ac_fn_c_check_func ()
-{
-  as_lineno=${as_lineno-"$1"} as_lineno_stack=as_lineno_stack=$as_lineno_stack
-  { $as_echo "$as_me:${as_lineno-$LINENO}: checking for $2" >&5
-$as_echo_n "checking for $2... " >&6; }
-if eval \${$3+:} false; then :
-  $as_echo_n "(cached) " >&6
-else
-  cat confdefs.h - <<_ACEOF >conftest.$ac_ext
-/* end confdefs.h.  */
-/* Define $2 to an innocuous variant, in case <limits.h> declares $2.
-   For example, HP-UX 11i <limits.h> declares gettimeofday.  */
-#define $2 innocuous_$2
-
-/* System header to define __stub macros and hopefully few prototypes,
-    which can conflict with char $2 (); below.
-    Prefer <limits.h> to <assert.h> if __STDC__ is defined, since
-    <limits.h> exists even on freestanding compilers.  */
-
-#ifdef __STDC__
-# include <limits.h>
-#else
-# include <assert.h>
-#endif
-
-#undef $2
-
-/* Override any GCC internal prototype to avoid an error.
-   Use char because int might match the return type of a GCC
-   builtin and then its argument prototype would still apply.  */
-#ifdef __cplusplus
-extern "C"
-#endif
-char $2 ();
-/* The GNU C library defines this for functions which it implements
-    to always fail with ENOSYS.  Some functions are actually named
-    something starting with __ and the normal name is an alias.  */
-#if defined __stub_$2 || defined __stub___$2
-choke me
-#endif
-
-int
-main ()
-{
-return $2 ();
-  ;
-  return 0;
-}
-_ACEOF
-if ac_fn_c_try_link "$LINENO"; then :
-  eval "$3=yes"
-else
-  eval "$3=no"
-fi
-rm -f core conftest.err conftest.$ac_objext \
-    conftest$ac_exeext conftest.$ac_ext
-fi
-eval ac_res=\$$3
-	       { $as_echo "$as_me:${as_lineno-$LINENO}: result: $ac_res" >&5
-$as_echo "$ac_res" >&6; }
-  eval $as_lineno_stack; ${as_lineno_stack:+:} unset as_lineno
-
-} # ac_fn_c_check_func
 cat >config.log <<_ACEOF
 This file contains any messages produced by compilers while
 running configure, to aid debugging if configure makes a mistake.
@@ -5680,6 +5680,57 @@

 fi

+
+ac_fn_c_check_func "$LINENO" "xdr_double" "ac_cv_func_xdr_double"
+if test "x$ac_cv_func_xdr_double" = xyes; then :
+
+else
+  { $as_echo "$as_me:${as_lineno-$LINENO}: checking for xdr_double in -ltirpc" >&5
+$as_echo_n "checking for xdr_double in -ltirpc... " >&6; }
+if ${ac_cv_lib_tirpc_xdr_double+:} false; then :
+  $as_echo_n "(cached) " >&6
+else
+  ac_check_lib_save_LIBS=$LIBS
+LIBS="-ltirpc  $LIBS"
+cat confdefs.h - <<_ACEOF >conftest.$ac_ext
+/* end confdefs.h.  */
+
+/* Override any GCC internal prototype to avoid an error.
+   Use char because int might match the return type of a GCC
+   builtin and then its argument prototype would still apply.  */
+#ifdef __cplusplus
+extern "C"
+#endif
+char xdr_double ();
+int
+main ()
+{
+return xdr_double ();
+  ;
+  return 0;
+}
+_ACEOF
+if ac_fn_c_try_link "$LINENO"; then :
+  ac_cv_lib_tirpc_xdr_double=yes
+else
+  ac_cv_lib_tirpc_xdr_double=no
+fi
+rm -f core conftest.err conftest.$ac_objext \
+    conftest$ac_exeext conftest.$ac_ext
+LIBS=$ac_check_lib_save_LIBS
+fi
+{ $as_echo "$as_me:${as_lineno-$LINENO}: result: $ac_cv_lib_tirpc_xdr_double" >&5
+$as_echo "$ac_cv_lib_tirpc_xdr_double" >&6; }
+if test "x$ac_cv_lib_tirpc_xdr_double" = xyes; then :
+  TLIBS="$TLIBS -ltirpc"
+else
+  as_fn_error $? "Need xdr_double" "$LINENO" 5
+fi
+
+fi
+
+
+
 #AC_CONFIG_SUBDIRS($MY_SUBDIRS)

 # Find where Data begins.  This is used by the storage allocation


Take care,

Leon Dutoit writes:

>
> Greetings!  Can you post a failing build log please?
>
> Take care,
>
>> Compilation of axiom fails on slackware64 (current) because in glibc-2.14 support of rpc has been suppressed.
>>
>> The problem is with gcl-2.6.8pre4 (and maybe after).
>>
>> To have rpc support the library "libtiprc" must be installed, but then at the link stage "-ltirpc" should be added.
>> This is apparently not checked in the configure script in gcl-2.6.8pre4.

>
> I get this :
>
> touch raw_pre_gcl_map
> gcc -o raw_pre_gcl /d1/axiom/obj/slackware/lib/cfuns-c.o /d1/axiom/obj/sl=
ackware/lib/sockio-c.o \
>         -L.  -Wl,-Map raw_pre_gcl_map   -lpre_gcl -lm  -lc -lgclp /d1/axi=
om/obj/slackware/lib/libspad.a
> ./libpre_gcl.a(funlink.o): In function `siGxdr_open':
> funlink.c:(.text+0x112): undefined reference to `xdrstdio_create'
> ./libpre_gcl.a(funlink.o): In function `siGxdr_read':
> funlink.c:(.text+0x1e6): undefined reference to `xdr_double'
> funlink.c:(.text+0x206): undefined reference to `xdr_float'
> funlink.c:(.text+0x22e): undefined reference to `xdr_long'
> funlink.c:(.text+0x27a): undefined reference to `xdr_short'
> funlink.c:(.text+0x282): undefined reference to `xdr_float'
> funlink.c:(.text+0x2a6): undefined reference to `xdr_array'
> funlink.c:(.text+0x2c2): undefined reference to `xdr_double'
> funlink.c:(.text+0x2d2): undefined reference to `xdr_long'
> ./libpre_gcl.a(funlink.o): In function `siGxdr_write':
> funlink.c:(.text+0x385): undefined reference to `xdr_double'
> funlink.c:(.text+0x3a5): undefined reference to `xdr_float'
> funlink.c:(.text+0x3cd): undefined reference to `xdr_long'
> funlink.c:(.text+0x402): undefined reference to `xdr_short'
> funlink.c:(.text+0x412): undefined reference to `xdr_float'
> funlink.c:(.text+0x436): undefined reference to `xdr_array'
> funlink.c:(.text+0x452): undefined reference to `xdr_long'
> funlink.c:(.text+0x462): undefined reference to `xdr_double'
> collect2: ld returned 1 exit status
> make[4]: *** [raw_pre_gcl_map] Error 1
> make[4]: Leaving directory `/d1/axiom/lsp/gcl-2.6.8pre4/unixport'
> make[3]: *** [unixport/saved_pre_gcl] Error 2
> make[3]: Leaving directory `/d1/axiom/lsp/gcl-2.6.8pre4'
> /bin/sh: line 5: unixport/saved_gcl: No such file or directory
> make[2]: *** [gcldir] Error 127
> make[2]: Leaving directory `/d1/axiom/lsp'
> make[1]: *** [lspdir] Error 2
> make[1]: Leaving directory `/d1/axiom'
> make: *** [all] Error 2

\start
Date: Mon, 09 Jan 2012 07:12:56 -0600
From: Gabriel Dos Reis
To: list
Subject: CICM 2012: Second call for paper

Apologies for duplicate copies.

         CICM 2012 - Conference on Intelligent Computer Mathematics
           July 9-13, 2012 at Jacobs University, Bremen, Germany

              http://www.informatik.uni-bremen.de/cicm2012/

                             Call for Papers
----------------------------------------------------------------

As computers and communications technology advance, greater
opportunities arise for intelligent mathematical computation. While
computer algebra, automated deduction, mathematical publishing and
novel user interfaces individually have long and successful histories,
we are now seeing increasing opportunities for synergy among these
areas. The Conference on Intelligent Computer Mathematics offers a
venue for discussing these areas and their synergy.

The conference will be organized by Serge Autexier and Michael
Kohlhase at Jacobs University in Bremen and consist of five tracks:

Artificial Intelligence and Symbolic Computation (AISC)
  Co-Chairs: John A. Campbell, Jacques Carette
Calculemus
  Chair: Gabriel Dos Reis
Digital Mathematical Libraries (DML)
  Chair: Petr Sojka
Mathematical Knowledge Management (MKM)
  Chair: Makarius Wenzel
Systems and Projects
  Chair: Volker Sorge

The overall programme will be organized by the General Program Chair
Johan Jeuring.

Invited talks will be given by:

Yannis Haralambous, D=C3=A9partement Informatique, T=C3=A9l=C3=A9com Bretagne
Conor McBride, Department of Computer and Information Sciences, University of Strathclyde
Cezar Ionescu, Potsdam Institute for Climate Impact Research

----------------------------------------------------------------
                             Important dates
----------------------------------------------------------------

Abstract submission:          20 February 2012
Submission deadline:          26 February 2012
Reviews sent to authors:     23 March 2012
Rebuttals due:                     30 March 2012
Notification of acceptance: 6  April 2012
Camera ready copies due:   20 April 2012
Conference:                         9-13 July 2012

----------------------------------------------------------------
                               Tracks
----------------------------------------------------------------

*** AISC ***

Symbolic computation can be roughly described as the study of
algorithms which operate on expression trees. Another way to phrase
this is to say that the denotational semantics of expressions trees is
not fixed, but is rather context dependent. Expression simplification
is probably the archetypal symbolic computation. Mathematically
oriented software (such as the so-called computer algebra systems)
have been doing this for decades, but not long thereafter, systems
doing proof planning and theorem discovery also started doing the
same; some attempts at knowledge management and 'expert systems' were
also symbolic, but less successfully so. More recently, many different
kinds of program analyses have gotten `symbolic', as well as some of
the automated theorem proving (SMT, CAV, etc).

But a large number of the underlying problems solved by symbolic
techniques are well known to be undecidable (never mind the many that
are EXP-time complete, etc). Artificial Intelligence has been
attacking many of these different sub-problems for quite some time,
and has also built up a solid body of knowledge. In fact, most
symbolic computation systems grew out of AI projects.

These two fields definitely intersect. One could say that in the
intersection lies all those problems for which we have no decision
procedures. In other words, decision procedures mark a definite phase
shift in our understanding, but are not always possible. Yet we still
want to solve certain problems, and must find 'other' means of
(partial) solution. This is the fertile land which comprises the core
of AISC.

Rather than try to exhaustively list topics of interest, it is
simplest to say that AISC seeks work which advances the understanding
of

Solving problems which fundamentally involve the manipulation of
expressions, but for which decision procedures are unlikely to ever
exist.


*** Calculemus ***

Calculemus is a series of conferences dedicated to the integration of
computer algebra systems (CAS) and systems for mechanised reasoning,
the interactive theorem provers or proof assistants (PA) and the
automated theorem provers (ATP). Currently, symbolic computation is
divided into several (more or less) independent branches: traditional
ones (e.g., computer algebra and mechanised reasoning) as well as
newly emerging ones (on user interfaces, knowledge management, theory
exploration, etc.) The main concern of the Calculemus community is to
bring these developments together in order to facilitate the theory,
design, and implementation of integrated systems for computer
mathematics that will routinely be used by mathematicians, computer
scientists and engineers in their every day business.

The topics of interest of Calculemus include but are not limited to:

	* Theorem proving in computer algebra (CAS)
	* Computer algebra in theorem proving (PA and ATP)
	* Case studies and applications that both involve computer
         	algebra and mechanised reasoning
	* Representation of mathematics in computer algebra
	* Adding computational capabilities to PA and ATP
	* Formal methods requiring mixed computing and proving
	* Combining methods of symbolic computation and formal
                 deduction
	* Mathematical computation in PA and ATP
	* Theory, design and implementation of interdisciplinary
        	systems for computer mathematics
	* Theory exploration techniques
	* Input languages, programming languages, types and constraint
    	        languages, and modeling languages for mechanised
    	        mathematics systems (PA, CAS, and ATP).
	* Infrastructure for mathematical services

*** DML ***

Mathematicians dream of a digital archive containing all peer-reviewed
mathematical literature ever published, properly linked, validated and
verified.  It is estimated that the entire corpus of mathematical
knowledge published over the centuries does not exceed 100,000,000
pages, an amount easily manageable by current information
technologies. Following success of DML 2008, DML 2009 DML 2010, and
DML 2011 track objectives are to formulate the strategy and goals of a
global mathematical digital library and to summarize the current
successes and failures of ongoing technologies and related projects as
EuDML, asking such questions as:

	* What technologies, standards, algorithms and formats should
		be used and what metadata should be shared?
	* What business models are suitable for publishers of
		mathematical literature, authors and funders of their
		projects and institutions?
	* Is there a model of sustainable, interoperable, and
		extensible mathematical library that mathematicians
		can use in their everyday work?
	* What is the best practice for
		* retrodigitized mathematics (from images via OCR to
                         MathML or TeX);
		* retro-born-digital mathematics (from existing
			electronic copy in DVI, PS or PDF to MathML or
			TeX);
		* born-digital mathematics (how to make needed
			metadata and file formats available as a side
			effect of publishing workflow [CEDRAM/Euclid
			model])?

DML is an opportunity to share experience and best practices between
projects in any area (MKM, NLP, OCR, pattern recognition, whatever)
that could change the paradigm for searching, accessing, and
interacting with the mathematical corpus. The track is
trans/interdisciplinary and contributions from any kind of people on
any aspect of the DML building are welcome.

*** MKM ***

Mathematical Knowledge Management is an interdisciplinary field of
research in the intersection of mathematics, computer science, library
science, and scientific publishing. The objective of MKM is to develop
new and better ways of managing sophisticated mathematical knowledge,
based on innovative technology of computer science, the Internet, and
intelligent knowledge processing. MKM is expected to serve
mathematicians, scientists, and engineers who produce and use
mathematical knowledge; educators and students who teach and learn
mathematics; publishers who offer mathematical textbooks and
disseminate new mathematical results; and librarians and
mathematicians who catalog and organize mathematical knowledge.

The conference is concerned with all aspects of mathematical knowledge
management. A non-exclusive list of important topics includes:

	* Representations of mathematical knowledge
	* Authoring languages and tools
	* Repositories of formalized mathematics
	* Deduction systems
	* Mathematical digital libraries
	* Diagrammatic representations
	* Mathematical OCR
	* Mathematical search and retrieval
	* Math assistants, tutoring and assessment systems
	* MathML, OpenMath, and other mathematical content standards
	* Web presentation of mathematics
	* Data mining, discovery, theory exploration
	* Computer algebra systems
	* Collaboration tools for mathematics
	* Challenges and solutions for mathematical workflows

*** Systems and Projects ***

The Systems and Projects track of the Conferences on Intelligent
Computer Mathematics is a forum for presentation of systems and new
and ongoing projects in all areas and topics related to the CICM
conferences:

	* AI and Symbolic Computation
	* Deduction and Computer Algebra
	* Mathematical Knowledge Management
	* Digital Mathematical Libraries

The track aims to provide an overview of the latest developments and
trends within the CICM community as well as to exchange ideas between
developers and introduce systems to an audience of potential users.

We solicit submissions for two page abstracts in the categories of
system descriptions and project presentations. System description
should present

	* newly developed systems,
	* systems that have not previously been presented to the CICM
                 community, or
	* significant updates to existing systems.

Project presentation should describe

	* projects that are new or about to start,
	* ongoing projects that have not yet been presented to the
                 CICM community.
	* significant new developments in ongoing previously presented
                 projects.

All submissions should contain links to demos, downloadable systems,
or project pages. Availability of such accompanying material will be a
strong prerequisite for acceptance.

Accepted abstracts will be published in the CICM proceedings in
Springer's LNAI series. Author's are expected to present their
abstracts in 5-10 minute teaser talks followed by an open demo/poster
session. System papers must be accompanied by a system demonstration,
while project papers must be accompanied by a poster presentation.

----------------------------------------------------------------
                              Submitting
----------------------------------------------------------------

Submissions to tracks A to D must not exceed 15 pages and will be
reviewed and evaluated with respect to relevance, clarity, quality,
originality, and impact.  Shorter papers, e.g., for system
descriptions, are welcome. Authors will have an opportunity to respond
to their papers' reviews before the programme committee makes a
decision.

Submissions to the Systems & Projects track must not exceed four
pages. The accepted abstracts will be presented at CICM in a fast
presentation session, followed by an open demo/poster session. System
papers must be accompanied by a system demonstration, and project
papers must be accompanied by a poster presentation. The four pages of
the abstract should be new material, accompanied by links to
demos/downloads/project-pages and [existing] system descriptions.
Availability of such accompanying material will be a strong
prerequisite for acceptance.

Accepted conference submissions from all tracks will be published as a
volume in the series Lecture Notes in Artificial Intelligence (LNAI)
by Springer. In addition to these formal proceedings, authors are
permitted and encouraged to publish the final versions of their papers
on arXiv.org.

Work-in-progress submissions are intended to provide a forum for the
presentation of original work that is not (yet) in a suitable form for
submission as a full or system description paper. This includes work
in progress and emerging trends. Their size is not limited, but we
recommend 5 - 10 pages.

The programme committee may offer authors of rejected formal
submissions to publish their contributions as work-in-progress papers
instead. Depending on the number of work-in-progress papers accepted,
they will be presented at the conference either as short talks or as
posters. The work-in-progress proceedings will be published as a
technical report.

All papers should be prepared in LaTeX and formatted according to the
requirements of Springer's LNCS series (the corresponding style files
can be downloaded from
http://www.springer.de/comp/lncs/authors.html). By submitting a paper
the authors agree that if it is accepted at least one of the authors
will attend the conference to present it.

Electronic submission is done through easychair
(http://www.easychair.org/conferences/?conf=cicm2012).

----------------------------------------------------------------
                            Program Committees
----------------------------------------------------------------

General chair: Johan Jeuring (Utrecht University and Open Universiteit
the Netherlands)

AISC track
	John A. Campbell; University College London, UK; Co-chair
	Jacques Carette; McMaster University, Canada; Co-chair
       Serge Autexier; DFKI Bremen, Germany
       Jacques Calmet; University of Karlsruhe, Germany
       Jacques Fleuriot; University of Edinburgh, UK
       Andrea Kohlhase; International University Bremen, Germany
       Erik Postma; Maplesoft Inc., Canada
       Alan Sexton; University of Birmingham, UK
       Chung-chieh Shan; Cornell University, USA.
       Stephen Watt; University of Western Ontario, Canada

Calculemus track
	Gabriel Dos Reis; Texas A&M University, USA; Chair
        Andrea Asperti; University of Bologna, Italy
        Laurent Bernardin; Maplesoft, Canada
        James Davenport; University of Bath, UK
        Ruben Gamboa; University of Wyoming, USA
        Mark Giesbrecht; University of Waterloo, Canada
        Sumit Gulwani; Microsoft Research, USA
        John Harrison; Intel, USA
        Joris van der Hoeven; =C3=89cole Polytechnique, France
        Hoon Hong; North Carolina State University, USA
        Lo=C3=AFc Pottier; INRIA, France
        Wolfgang Windsteiger; RISC, Austria

DML track
        Petr Sojka; Masaryk University, Brno, CZ; Chair
         Jos=C3=A9 Borbinha; Technical University of Lisbon,  PT
        Thierry Bouche; University Grenoble, FR
        Michael Doob; University of Manitoba, Winnipeg, CA
        Thomas Fischer; Goettingen University,  DE
        Yannis Haralambous; T=C3=A9l=C3=A9com Bretagne, FR
        V=C3=A1clav Hlav=C3=A1=C4=8D; Czech Technical University, Prague, CZ
        Michael Kohlhase; Jacobs University Bremen, DE
        Janka Chleb=C3=ADkov=C3=A1; Portsmouth University, UK
        Enrique Maci=C3=A1s-Virg=C3=B3s; University of Santiago de Composte=
la,
          ES
        Bruce Miller; NIST, USA
        Ji=C5=99=C3=AD R=C3=A1kosn=C3=ADk; Academy of Sciences, Prague, CZ
        Eugenio Rocha; University of Aveiro, PT
        David Ruddy; Cornell University, US
        Volker Sorge; University of Birmingham, UK
        Masakazu Suzuki; Kyushu University, JP

MKM track
	Makarius Wenzel; University of Paris-South, France; Chair
        David Aspinall; University of Edinburgh, Scotland
        Jeremy Avigad; Carnegie Mellon University, USA
        Mateja Jamnik; University of Cambridge, UK
        Cezary Kaliszyk; University of Tsukuba, Japan
        Manfred Kerber; University of Birmingham, UK
        Christoph L=C3=BCth; DFKI Bremen, Germany
        Adam Naumowicz; University of Bia=C5=82ystok, Poland
        Jim Pitman; University of California, Berkeley, USA
        Pedro Quaresma; Universidade de Coimbra, Portugal
        Florian Rabe; Jacobs University Bremen, Germany
        Claudio Sacerdoti Coen; University of Bologna, Italy
        Enrico Tassi; INRIA Saclay, France
        Freek Wiedijk; Radboud University Nijmegen, The Netherlands

Systems & Projects track
	Volker Sorge; University of Birmingham, UK; Chair
        Josef Baker; University of Birmingham, UK
        John Charnley; Imperial College, UK
        Manuel Kauers; RISC, Austria
        Koji Nakagawa; Kyushu University, Japan
        Piotr Rudnicki; University of Alberta, Canada
        Josef Urban; Radboud University Nijmegen, The Netherlands
        Richard Zanibbi; Rochester Institute of Technologies, USA

\start
Date: Mon, 09 Jan 2012 09:30:30 -0500
From: Tim Daly
To: list
Subject: CICM 2012 - Conference on Intelligent Computer	Mathematics

         CICM 2012 - Conference on Intelligent Computer Mathematics
           July 9-13, 2012 at Jacobs University, Bremen, Germany

              http://www.informatik.uni-bremen.de/cicm2012/
 
                             Call for Papers
----------------------------------------------------------------

As computers and communications technology advance, greater
opportunities arise for intelligent mathematical computation. While
computer algebra, automated deduction, mathematical publishing and
novel user interfaces individually have long and successful histories,
we are now seeing increasing opportunities for synergy among these
areas. The Conference on Intelligent Computer Mathematics offers a
venue for discussing these areas and their synergy.

The conference will be organized by Serge Autexier and Michael
Kohlhase at Jacobs University in Bremen and consist of five tracks:

Artificial Intelligence and Symbolic Computation (AISC)
  Co-Chairs: John A. Campbell, Jacques Carette
Calculemus
  Chair: Gabriel Dos Reis
Digital Mathematical Libraries (DML)
  Chair: Petr Sojka
Mathematical Knowledge Management (MKM)
  Chair: Makarius Wenzel
Systems and Projects
  Chair: Volker Sorge
 
The overall programme will be organized by the General Program Chair
Johan Jeuring.

Invited talks will be given by:

Yannis Haralambous, Departement Informatique, Tom Bretagne
Conor McBride, Department of Computer and Information Sciences,
University of Strathclyde
Cezar Ionescu, Potsdam Institute for Climate Impact Research

----------------------------------------------------------------
                             Important dates
----------------------------------------------------------------

Abstract submission:          20 February 2012
Submission deadline:          26 February 2012
Reviews sent to authors:     23 March 2012
Rebuttals due:                     30 March 2012
Notification of acceptance: 6  April 2012
Camera ready copies due:   20 April 2012
Conference:                         9-13 July 2012

----------------------------------------------------------------
                               Tracks
----------------------------------------------------------------

*** AISC ***

Symbolic computation can be roughly described as the study of
algorithms which operate on expression trees. Another way to phrase
this is to say that the denotational semantics of expressions trees is
not fixed, but is rather context dependent. Expression simplification
is probably the archetypal symbolic computation. Mathematically
oriented software (such as the so-called computer algebra systems)
have been doing this for decades, but not long thereafter, systems
doing proof planning and theorem discovery also started doing the
same; some attempts at knowledge management and 'expert systems' were
also symbolic, but less successfully so. More recently, many different
kinds of program analyses have gotten `symbolic', as well as some of
the automated theorem proving (SMT, CAV, etc).

But a large number of the underlying problems solved by symbolic
techniques are well known to be undecidable (never mind the many that
are EXP-time complete, etc). Artificial Intelligence has been
attacking many of these different sub-problems for quite some time,
and has also built up a solid body of knowledge. In fact, most
symbolic computation systems grew out of AI projects.

These two fields definitely intersect. One could say that in the
intersection lies all those problems for which we have no decision
procedures. In other words, decision procedures mark a definite phase
shift in our understanding, but are not always possible. Yet we still
want to solve certain problems, and must find 'other' means of
(partial) solution. This is the fertile land which comprises the core
of AISC.

Rather than try to exhaustively list topics of interest, it is
simplest to say that AISC seeks work which advances the understanding
of

Solving problems which fundamentally involve the manipulation of
expressions, but for which decision procedures are unlikely to ever
exist.


*** Calculemus ***

Calculemus is a series of conferences dedicated to the integration of
computer algebra systems (CAS) and systems for mechanised reasoning,
the interactive theorem provers or proof assistants (PA) and the
automated theorem provers (ATP). Currently, symbolic computation is
divided into several (more or less) independent branches: traditional
ones (e.g., computer algebra and mechanised reasoning) as well as
newly emerging ones (on user interfaces, knowledge management, theory
exploration, etc.) The main concern of the Calculemus community is to
bring these developments together in order to facilitate the theory,
design, and implementation of integrated systems for computer
mathematics that will routinely be used by mathematicians, computer
scientists and engineers in their every day business.

The topics of interest of Calculemus include but are not limited to:

        * Theorem proving in computer algebra (CAS)
        * Computer algebra in theorem proving (PA and ATP)
        * Case studies and applications that both involve computer
                algebra and mechanised reasoning
        * Representation of mathematics in computer algebra
        * Adding computational capabilities to PA and ATP
        * Formal methods requiring mixed computing and proving
        * Combining methods of symbolic computation and formal
                 deduction
        * Mathematical computation in PA and ATP
        * Theory, design and implementation of interdisciplinary
                systems for computer mathematics
        * Theory exploration techniques
        * Input languages, programming languages, types and constraint
                languages, and modeling languages for mechanised
                mathematics systems (PA, CAS, and ATP).
        * Infrastructure for mathematical services

*** DML ***

Mathematicians dream of a digital archive containing all peer-reviewed
mathematical literature ever published, properly linked, validated and
verified.  It is estimated that the entire corpus of mathematical
knowledge published over the centuries does not exceed 100,000,000
pages, an amount easily manageable by current information
technologies. Following success of DML 2008, DML 2009 DML 2010, and
DML 2011 track objectives are to formulate the strategy and goals of a
global mathematical digital library and to summarize the current
successes and failures of ongoing technologies and related projects as
EuDML, asking such questions as:

        * What technologies, standards, algorithms and formats should
                be used and what metadata should be shared?
        * What business models are suitable for publishers of
                mathematical literature, authors and funders of their
                projects and institutions?
        * Is there a model of sustainable, interoperable, and
                extensible mathematical library that mathematicians
                can use in their everyday work?
        * What is the best practice for
                * retrodigitized mathematics (from images via OCR to
                         MathML or TeX);
                * retro-born-digital mathematics (from existing
                        electronic copy in DVI, PS or PDF to MathML or
                        TeX);
                * born-digital mathematics (how to make needed
                        metadata and file formats available as a side
                        effect of publishing workflow [CEDRAM/Euclid
                        model])?

DML is an opportunity to share experience and best practices between
projects in any area (MKM, NLP, OCR, pattern recognition, whatever)
that could change the paradigm for searching, accessing, and
interacting with the mathematical corpus. The track is
trans/interdisciplinary and contributions from any kind of people on
any aspect of the DML building are welcome.

*** MKM ***

Mathematical Knowledge Management is an interdisciplinary field of
research in the intersection of mathematics, computer science, library
science, and scientific publishing. The objective of MKM is to develop
new and better ways of managing sophisticated mathematical knowledge,
based on innovative technology of computer science, the Internet, and
intelligent knowledge processing. MKM is expected to serve
mathematicians, scientists, and engineers who produce and use
mathematical knowledge; educators and students who teach and learn
mathematics; publishers who offer mathematical textbooks and
disseminate new mathematical results; and librarians and
mathematicians who catalog and organize mathematical knowledge.

The conference is concerned with all aspects of mathematical knowledge
management. A non-exclusive list of important topics includes:

        * Representations of mathematical knowledge
        * Authoring languages and tools
        * Repositories of formalized mathematics
        * Deduction systems
        * Mathematical digital libraries
        * Diagrammatic representations
        * Mathematical OCR
        * Mathematical search and retrieval
        * Math assistants, tutoring and assessment systems
        * MathML, OpenMath, and other mathematical content standards
        * Web presentation of mathematics
        * Data mining, discovery, theory exploration
        * Computer algebra systems
        * Collaboration tools for mathematics
        * Challenges and solutions for mathematical workflows
       
*** Systems and Projects ***

The Systems and Projects track of the Conferences on Intelligent
Computer Mathematics is a forum for presentation of systems and new
and ongoing projects in all areas and topics related to the CICM
conferences:

        * AI and Symbolic Computation
        * Deduction and Computer Algebra
        * Mathematical Knowledge Management
        * Digital Mathematical Libraries

The track aims to provide an overview of the latest developments and
trends within the CICM community as well as to exchange ideas between
developers and introduce systems to an audience of potential users.

We solicit submissions for two page abstracts in the categories of
system descriptions and project presentations. System description
should present

        * newly developed systems,
        * systems that have not previously been presented to the CICM
                 community, or
        * significant updates to existing systems.

Project presentation should describe

        * projects that are new or about to start,
        * ongoing projects that have not yet been presented to the
                 CICM community.
        * significant new developments in ongoing previously presented
                 projects.

All submissions should contain links to demos, downloadable systems,
or project pages. Availability of such accompanying material will be a
strong prerequisite for acceptance.

Accepted abstracts will be published in the CICM proceedings in
Springer's LNAI series. Author's are expected to present their
abstracts in 5-10 minute teaser talks followed by an open demo/poster
session. System papers must be accompanied by a system demonstration,
while project papers must be accompanied by a poster presentation.

----------------------------------------------------------------
                              Submitting
----------------------------------------------------------------

Submissions to tracks A to D must not exceed 15 pages and will be
reviewed and evaluated with respect to relevance, clarity, quality,
originality, and impact.  Shorter papers, e.g., for system
descriptions, are welcome. Authors will have an opportunity to respond
to their papers' reviews before the programme committee makes a
decision.

Submissions to the Systems & Projects track must not exceed four
pages. The accepted abstracts will be presented at CICM in a fast
presentation session, followed by an open demo/poster session. System
papers must be accompanied by a system demonstration, and project
papers must be accompanied by a poster presentation. The four pages of
the abstract should be new material, accompanied by links to
demos/downloads/project-pages and [existing] system descriptions.
Availability of such accompanying material will be a strong
prerequisite for acceptance.

Accepted conference submissions from all tracks will be published as a
volume in the series Lecture Notes in Artificial Intelligence (LNAI)
by Springer. In addition to these formal proceedings, authors are
permitted and encouraged to publish the final versions of their papers
on arXiv.org.

Work-in-progress submissions are intended to provide a forum for the
presentation of original work that is not (yet) in a suitable form for
submission as a full or system description paper. This includes work
in progress and emerging trends. Their size is not limited, but we
recommend 5 - 10 pages.

The programme committee may offer authors of rejected formal
submissions to publish their contributions as work-in-progress papers
instead. Depending on the number of work-in-progress papers accepted,
they will be presented at the conference either as short talks or as
posters. The work-in-progress proceedings will be published as a
technical report.

All papers should be prepared in LaTeX and formatted according to the
requirements of Springer's LNCS series (the corresponding style files
can be downloaded from
http://www.springer.de/comp/lncs/authors.html). By submitting a paper
the authors agree that if it is accepted at least one of the authors
will attend the conference to present it.

Electronic submission is done through easychair
(http://www.easychair.org/conferences/?conf=cicm2012).

----------------------------------------------------------------
                            Program Committees
----------------------------------------------------------------

General chair: Johan Jeuring (Utrecht University and Open Universiteit
the Netherlands)

AISC track
        John A. Campbell; University College London, UK; Co-chair
        Jacques Carette; McMaster University, Canada; Co-chair
       Serge Autexier; DFKI Bremen, Germany
       Jacques Calmet; University of Karlsruhe, Germany
       Jacques Fleuriot; University of Edinburgh, UK
       Andrea Kohlhase; International University Bremen, Germany
       Erik Postma; Maplesoft Inc., Canada
       Alan Sexton; University of Birmingham, UK
       Chung-chieh Shan; Cornell University, USA.
       Stephen Watt; University of Western Ontario, Canada

Calculemus track
        Gabriel Dos Reis; Texas A&M University, USA; Chair
        Andrea Asperti; University of Bologna, Italy
        Laurent Bernardin; Maplesoft, Canada
        James Davenport; University of Bath, UK
        Ruben Gamboa; University of Wyoming, USA
        Mark Giesbrecht; University of Waterloo, Canada
        Sumit Gulwani; Microsoft Research, USA
        John Harrison; Intel, USA
        Joris van der Hoeven; =C3=89cole Polytechnique, France
        Hoon Hong; North Carolina State University, USA
        Lo=C3=AFc Pottier; INRIA, France
        Wolfgang Windsteiger; RISC, Austria

DML track
        Petr Sojka; Masaryk University, Brno, CZ; Chair
         Jos=C3=A9 Borbinha; Technical University of Lisbon,  PT
        Thierry Bouche; University Grenoble, FR
        Michael Doob; University of Manitoba, Winnipeg, CA
        Thomas Fischer; Goettingen University,  DE
        Yannis Haralambous; T=C3=A9l=C3=A9com Bretagne, FR
        V=C3=A1clav Hlav=C3=A1=C4=8D; Czech Technical University, Prague,=
 CZ
        Michael Kohlhase; Jacobs University Bremen, DE
        Janka Chleb=C3=ADkov=C3=A1; Portsmouth University, UK
        Enrique Maci=C3=A1s-Virg=C3=B3s; University of Santiago de Compos=
tela,
          ES
        Bruce Miller; NIST, USA
        Ji=C5=99=C3=AD R=C3=A1kosn=C3=ADk; Academy of Sciences, Prague, C=
Z
        Eugenio Rocha; University of Aveiro, PT
        David Ruddy; Cornell University, US
        Volker Sorge; University of Birmingham, UK
        Masakazu Suzuki; Kyushu University, JP

MKM track
        Makarius Wenzel; University of Paris-South, France; Chair
        David Aspinall; University of Edinburgh, Scotland
        Jeremy Avigad; Carnegie Mellon University, USA
        Mateja Jamnik; University of Cambridge, UK
        Cezary Kaliszyk; University of Tsukuba, Japan
        Manfred Kerber; University of Birmingham, UK
        Christoph L=C3=BCth; DFKI Bremen, Germany
        Adam Naumowicz; University of Bia=C5=82ystok, Poland
        Jim Pitman; University of California, Berkeley, USA
        Pedro Quaresma; Universidade de Coimbra, Portugal
        Florian Rabe; Jacobs University Bremen, Germany
        Claudio Sacerdoti Coen; University of Bologna, Italy
        Enrico Tassi; INRIA Saclay, France
        Freek Wiedijk; Radboud University Nijmegen, The Netherlands

Systems & Projects track
        Volker Sorge; University of Birmingham, UK; Chair
        Josef Baker; University of Birmingham, UK
        John Charnley; Imperial College, UK
        Manuel Kauers; RISC, Austria
        Koji Nakagawa; Kyushu University, Japan
        Piotr Rudnicki; University of Alberta, Canada
        Josef Urban; Radboud University Nijmegen, The Netherlands
        Richard Zanibbi; Rochester Institute of Technologies, USA

\start
Date: Tue, 10 Jan 2012 16:04:32 -0500
From: Camm Maguire
To: Tim Daly
Subject: 20111101

Greetings!  Is there a *very* long regression test now in richhyper1000-1098?

\start
Date: Tue, 10 Jan 2012 21:26:52 -0500
From: Tim Daly
To: list
Subject: Where literate programming could have a huge impact

Publish the code and the data as well as the results.
People should be able to reproduce results without
contacting the original authors.

We really do need to raise the level of scholarship
in software and software-related science.

Scientists, Share Secrets or Lose Funding: Stodden and Arbesman
2012-01-10 00:00:13.0 GMT


     (For more Bloomberg View, click on VIEW <GO>.)

By Victoria Stodden and Samuel Arbesman
     Jan. 10 (Bloomberg) -- The Journal of Irreproducible
Results, a science-humor magazine, is, sadly, no longer the only
publication that can lay claim to its title. More and more
published scientific studies are difficult or impossible to
repeat.
     It=E2=80=99s not that the experiments themselves are so flawed they
can=E2=80=99t be redone to the same effect -- though this happens more
than scientists would like. It=E2=80=99s that the data upon which the
work is based, as well as the methods employed, are too often
not published, leaving the science hidden.
     Many people assume that scientists the world over freely
exchange not only the results of their experiments but also the
detailed data, statistical tools and computer instructions they
employed to arrive at those results. This is the kind of
information that other scientists need in order to replicate the
studies. The truth is, open exchange of such information is not
common, making verification of published findings all but
impossible and creating a credibility crisis in computational
science.
     Federal agencies that fund scientific research are in a
position to help fix this problem. They should require that all
scientists whose studies they finance share the files that
generated their published findings, the raw data and the
computer instructions that carried out their analysis.
     The ability to reproduce experiments is important not only
for the advancement of pure science but also to address many
science-based issues in the public sphere, from climate change
to biotechnology.

                    Too Little Transparency

     Consider, for example, a recent notorious incident in
biomedical science. In 2006, researchers at Duke University
seemed to have discovered relationships between lung cancer
patients=E2=80=99 personal genetic signatures and their responsiveness
to certain drugs. The scientists published their results in
respected journals (the New England Journal of Medicine and
Nature Medicine), but only part of the genetic signature data
used in the studies was publicly available, and the computer
codes used to generate the findings were never revealed. This is
unfortunately typical for scientific publications.
     The Duke research was considered such a breakthrough that
other scientists quickly became interested in replicating it,
but because so much information was unavailable, it took three
years for them to uncover and publicize a number of very serious
errors in the published reports. Eventually, those reports were
retracted, and clinical trials based on the flawed results were
canceled.
     In response to this incident, the Institute of Medicine
convened a committee to review what data should appropriately be
revealed from genomics research that leads to clinical trials.
This committee is due to release its report early this year.
     Unfortunately, the research community rarely addresses the
problem of reproducibility so directly. Inadequate sharing is
common to all scientific domains that use computers in their
research today (most of science), and it hampers transparency.
     By making the underlying data and computer code
conveniently available, scientists could open a new era of
innovation and growth. In October, the White House released a
memorandum titled =E2=80=9CAccelerating Technology Transfer and
Commercialization of Federal Research in Support of High-Growth
Businesses,=E2=80=9D which outlines ways for federal funding agencies to
improve the rate of technology transfer from government-financed
laboratories to the private business sector.

                      Technology Transfer

     In this memo, President Barack Obama called on federal
agencies to measure the rate of technology transfer. To this
end, agencies such as the National Institutes of Health and the
National Science Foundation should require that scientists who
receive federal funds publish full results, including the data
they are based on and all the computer steps taken to reach
them. This could include providing links to Internet sites
containing the data and codes required to replicate the
published results.
     Exceptions could be made when necessary -- some information
might need to be kept confidential for national-security
reasons, for example. But standard practice for scientific
publication should be full transparency.
     Leaving this up to the scientific community isn=E2=80=99t
sufficient. Nor is relying on current federal rules. Grant
guidelines from the NIH and the NSF instruct researchers to
share with other investigators the data generated in the course
of their work, but this isn=E2=80=99t enforced. The NIH demands that
articles resulting from research it finances be made freely
available within a year of publication. But even if this policy
were extended to all government-financed studies, the data and
computer codes needed to verify the findings would still remain
inaccessible.
     As Jon Claerbout, a professor emeritus of geophysics at
Stanford University, has noted, scientific publication isn=E2=80=99t
scholarship itself, but only the advertising of scholarship. The
actual work -- the steps needed to reproduce the scientific
finding -- must be shared.
     Stricter requirements for transparency in publication would
allow scientific findings to more quickly become fuel for
innovation and help ensure that public policy is based on sound
science.


     (Victoria Stodden is an assistant professor of statistics
at Columbia University. Samuel Arbesman is a senior scholar at
the Ewing Marion Kauffman Foundation. The opinions expressed are
their own.)

\start
Date: Wed, 11 Jan 2012 12:20:27 -0500
From: Eugene Surowitz
To: list
Subject: Re: Where literate programming could have a huge impact

Suggested reading:

Computers in Science and Engineering (IEEE + APS):

vol.11 n.1 - most of the issue which is dedicated to the problem

vol.12 no.5 p.8--12 - A Yale round table on the subject

My impression is that many, if not most, labs would have trouble
reproducing their own results given propensities to modify code,
lack of archiving, wandering data sets, etc.

Then of course there is the instance of a group whose idea
of collaboration was to send a another researcher their raw data;
they sent her a four-inch thick fanfold binder of printout.

Eugene J. Surowitz

On 1/10/2012 9:26 PM, daly wrote:
> Publish the code and the data as well as the results.
> People should be able to reproduce results without
> contacting the original authors.
>
> We really do need to raise the level of scholarship
> in software and software-related science.
>
> Tim
>
>> From Alfredo Portes:
>
> +------------------------------------------------------------------=
------------+
>
> Scientists, Share Secrets or Lose Funding: Stodden and Arbesman
> 2012-01-10 00:00:13.0 GMT

\start
Date: Fri, 20 Jan 2012 10:57:32 +0000
From: Martin Baker
To: list
Subject: literate programming in html

Does the code have to be physically in the same document as the documentation 
to achieve the aims of literate programming?

The analogy that I would make is with graphics in html, these are stored in 
their own files so we can edit a .gif file or a .jpeg file or .png or .svg and 
so on each with there own appropriate graphics editor. The browser brings 
these all together, when its being read,  so that the end user can read the 
"story".

Is there some way this could be done for code? That is get the browser to 
insert it into the document when its being read rather than build what looks 
like quite messy tangling and untangling tools?

\start
Date: Fri, 20 Jan 2012 12:59:52 +0100
From: Ralf Hemmecke
To: Martin Baker
Subject: Re: literate programming in html

On 01/20/2012 11:57 AM, Martin Baker wrote:
> Does the code have to be physically in the same document as the documentation
> to achieve the aims of literate programming?

It all depends on your tools. If there were a tool that puts every code 
chunk into a separate file but while editing shows you the chunks in the 
order you want, you wouldn't care how your content is physically stored.

A tool that can show (maybe even different) views on code and 
documentation would certainly be optimal. Anyway, I think one of the 
important view is that a potential reader can understand the code and 
the ideas (theory) that the code builds on. So the "literate programmer" 
also has the task to linearize thoughts and help others to understand 
the program.

Different views on the program is another helpful feature, but without 
also restructuring the accompanying documentation, it would not belong 
to the world of LP.

LP doesn't say how things have to be stored. In only says that you 
embellish your documentation with nice constructive examples (the actual 
code).
(Traditionally, we embellish programs by documentation.)

\start
Date: Fri, 20 Jan 2012 10:24:22 -0500
From: Tim Daly
To: Martin Baker
Subject: Re: literate programming in html

On Fri, 2012-01-20 at 10:57 +0000, Martin Baker wrote:
> Does the code have to be physically in the same document as the documentation 
> to achieve the aims of literate programming?
> 
> The analogy that I would make is with graphics in html, these are stored in 
> their own files so we can edit a .gif file or a .jpeg file or .png or .svg and 
> so on each with there own appropriate graphics editor. The browser brings 
> these all together, when its being read,  so that the end user can read the 
> "story".
> 
> Is there some way this could be done for code? That is get the browser to 
> insert it into the document when its being read rather than build what looks 
> like quite messy tangling and untangling tools?

The example I gave was
http://axiom-developer.org/axiom-website/litprog.html
which was a literate program in HTML.

It only depends on the ability to find <pre> tags which
contain an id= field as in <pre id="chunkname">

If you have control of the server it would be possible to
use PHP to construct your documentation "on the fly" by
including source chunks and sending them to the final page.
You could put id fields on <a> tags and parse the href field.

Latex does allow you to \include files but that would be
missing the point. Latex can also re-render in html.

Unfortunately it feels like you are trying to maintain the
idea that source code should live in little files. This has
a very long history but is actually a very bad idea. We are
forced to create tools to handle this (e.g. IDEs) so we can
manipulate the program "as a whole". You want to go from the
little files to the big document. Literate programs are
generally structured to work from big to little.

Literate programs change the structure of files in very
fundamental ways. The first priority of a literate program
is to communicate to another human. The code is expanded
in the document when it is properly motivated. Thus you 
find that the contents of a header file is usually spread
around in a literate document and re-assembled for the
compiler.

\start
Date: Fri, 20 Jan 2012 10:26:03 -0500
From: Tim Daly
To: list
Subject: American Math Society column

Publishing Computational Mathematics
http://www.ams.org/notices/201202/rtx120200320p.pdf

\start
Date: Fri, 20 Jan 2012 10:38:44 -0500
From: Tim Daly
To: list
Subject: Excellent literate program example

There is a book called 
"Implementing Elliptic Curve Cryptography" by Mike Rosing
which is my new favorite literate program.

To really see the difference literate programming can make
you can download Rosing's program and you can read the book.
The program is standard C and contains lines like

   #define NUMBITS 158

Do you know why NUMBITS is 158? I doubt that you can 
reverse-engineer the answer from his code. But the book
makes the answer obvious.

Further, elliptic curve cryptography depends on things
like modular polynomial arithmetic which might be rather
hard for your average C programmer to reverse-engineer.
The book explains the theory.

I am trying to set up a study at some university that would
use Rosing's book and programs. I want to give one group a
copy of the book and the other group just the C programs.
Then I want to evaluate the comprehension of the two groups.

It is my conjecture that the book group would have a deeper
understanding of the program than the C group. I believe the
study will show this but until the study is done it is just
my opinion.

If anyone is at a university and knows of a professor who
might find such a study worthwhile, please let me know.
It is studies like this that make history in computer 
science, just like the studies of programmer productivity.
A properly done study would be a landmark paper.

\start
Date: Fri, 20 Jan 2012 15:43:26 +0000
From: Martin Baker
To: list
Subject: Re: literate programming in html

On Friday 20 Jan 2012 11:59:52 Ralf Hemmecke wrote:
> It all depends on your tools. If there were a tool that puts every code
> chunk into a separate file but while editing shows you the chunks in the
> order you want, you wouldn't care how your content is physically stored.

Its just that the method Tim describes for embedding code into html here:
http://axiom-developer.org/axiom-website/litprog.html
has some complications and requires specialised tools.

In some ways I find the current LP and hyperdoc stuff in Axiom flavors 
actually makes it harder for me to produce the quality of documentation that I 
would like, although moving to an html version like this would help, I still 
think custom tools like this are a major problem.

I think it would be much better to use standard tools. I'm not sure that file 
structure is the most important issue? This seems like a technical issue to 
me. In the end the important issues to me are:

* Can I use standard tools that I already know to edit code, html and diagrams 
in WYSIWYG mode.
* Does it require maintainence of specialised tools?
* Does it inhibit development of IDE capablitied like code completion and 
debugging tools?
* Does it make documentation harder to produce rather than easier?
* Is there duplication between LP, Hyperdoc, )show, etc.

I think there is a lot to be said for keeping the documentation (LP and 
hyperdoc) on a server somwhere and only downloading code to users client.

\start
Date: Fri, 20 Jan 2012 10:59:15 -0500
From: Tim Daly
To: Martin Baker
Subject: Re: literate programming in html

On Fri, 2012-01-20 at 15:43 +0000, Martin Baker wrote:
> On Friday 20 Jan 2012 11:59:52 Ralf Hemmecke wrote:
> > It all depends on your tools. If there were a tool that puts every code
> > chunk into a separate file but while editing shows you the chunks in the
> > order you want, you wouldn't care how your content is physically stored.
> 
> Its just that the method Tim describes for embedding code into html here:
> http://axiom-developer.org/axiom-website/litprog.html
> has some complications and requires specialised tools.
> 
> In some ways I find the current LP and hyperdoc stuff in Axiom flavors 
> actually makes it harder for me to produce the quality of documentation that I 
> would like, although moving to an html version like this would help, I still 
> think custom tools like this are a major problem.

Custom tools? The litprog.html describes a short C program that is the
only tool you need to do literate programming in HTML. And I've (almost
completely) rewritten Axiom to use standard latex everywhere so the
pamphlet files are straight latex using \begin{chunk}. The only tool
needed is a small tangle program which I have as a C utility and is
also compiled into the lisp image.

A single extraction tool does not seem to be much overhead.

> 
> I think it would be much better to use standard tools. I'm not sure that file 
> structure is the most important issue? This seems like a technical issue to 
> me. In the end the important issues to me are:
> 
> * Can I use standard tools that I already know to edit code, html and diagrams 
> in WYSIWYG mode.
> * Does it require maintainence of specialised tools?
> * Does it inhibit development of IDE capablitied like code completion and 
> debugging tools?
> * Does it make documentation harder to produce rather than easier?
> * Is there duplication between LP, Hyperdoc, )show, etc.
> 
> I think there is a lot to be said for keeping the documentation (LP and 
> hyperdoc) on a server somwhere and only downloading code to users client.
> 

Think of a calculus textbook. You want to keep the textbook on the 
server and only download the equations. That means that the user
will have a directory of equations without meaning living on the
local machine. I do not see the value of only downloading equations.

There is a language called Clojure. I have created a literate form
http://daly.axiom-developer.org/clojure.pamphlet
http://daly.axiom-developer.org/clojure.pdf

All of the information is in the pdf (and src) but basically you
extract a tangle program
http://daly.axiom-developer.org/tangle.c

If you do
   gcc -o tangle tangle.c
   tangle clojure.pamphlet Makefile >Makefile
   make

the makefile will extract the source code (mostly java) into the
correct tree, compile the code, run the test cases, rebuild the
pdf, and drop you into a running Clojure REPL.

If you modify the src pamphlet all you need to type is
    make clean
    make
and you now have a completely rebuilt, re-PDF'd, re-tested
Clojure with a running REPL. To modify the program and test
your new changes, just rinse and repeat.
 
How hard is this? It could hardly be easier. Send the single
pamphlet file to another user and they have everything.

\start
Date: Fri, 20 Jan 2012 18:54:25 +0000
From: Martin Baker
To: list
Subject: Re: literate programming in html

On Friday 20 Jan 2012 15:59:15 you wrote:
> How hard is this? It could hardly be easier. Send the single
> pamphlet file to another user and they have everything.

Tim,

Perhaps I'm too set in my ways or perhaps I'm missing something but I think we 
are going to have to agree to disagree on this issue.

I think we agree on the importance of documentation and the importance of 
explaining the reasons behind things and the aims of LP.

But I think we will have to agree to disagree on the mechanics and that there 
should only be one linear path through the documentation.

Perhaps my thinking comes from liking to understand things in terms of 
pictures and diagrams rather than big chunks of text (I suspect I differ from 
most people in the pan-Axiom community in this respect). So, what I'm looking 
for is documentation rich with pictures, diagrams, tables, lists, animations, 
hyperdoc-like capability, different layouts and so on. Not the equivalent of a 
very big book.

In order to do this I think the documentation has to be based on html + png + 
svg and so on. This is very different to existing pamphlet files, as far as I 
can see the Clojure example you give is not HTML?

I can't see that it would be very desirable to just put HTML tags into a mega 
pamphlet file and sort out the technical issues you mention in your document. 
For one thing, one does not want to edit low level HTML using a simple text 
editor, I think it really needs WYSIWYG html and graphics editors. Also the 
whole idea of hypertext seems to favour a more hierarchical approach rather 
than a linear document.

I want to be able to start with a quick introduction to the program and when I 
get to something I'm specially interested to click on a hyperlink and drill 
down to more detail. The navigation should give many ways to read the document 
including LP-like and Hyperdoc-like and )show-like (computer generated 
reference information) all richly interlinked.

We have discussed these things before and I think I'm starting to repeat 
myself so, as I say I think it best to agree to disagree. I will watch your 
experiment with interest and be ready to be proved wrong.

\start
Date: Fri, 20 Jan 2012 16:59:31 -0500
From: Tim Daly
To: list
Subject: Re: literate programming in html

On Fri, 2012-01-20 at 18:53 +0000, Martin Baker wrote:
> On Friday 20 Jan 2012 15:59:15 you wrote:
> > How hard is this? It could hardly be easier. Send the single
> > pamphlet file to another user and they have everything.
> 
> Tim,
> 
> Perhaps I'm too set in my ways or perhaps I'm missing something but I
think we 
> are going to have to agree to disagree on this issue.

Literate programming is a change of mindset. You clearly get it.
There is no "right" way to do it. And I don't feel we are disagreeing.

> 
> I think we agree on the importance of documentation and the importance
of 
> explaining the reasons behind things and the aims of LP.

Absolutely. And I think you are doing an excellent job.

> 
> But I think we will have to agree to disagree on the mechanics and
that there 
> should only be one linear path through the documentation.

Well I read a LOT and I find that non-linear paths leave me with the
feeling that I'm missing something important. I don't have a way to
mark my path so I know what other paths to follow.

> 
> Perhaps my thinking comes from liking to understand things in terms
of 
> pictures and diagrams rather than big chunks of text (I suspect I
differ from 
> most people in the pan-Axiom community in this respect). So, what I'm
looking 
> for is documentation rich with pictures, diagrams, tables, lists,
animations, 
> hyperdoc-like capability, different layouts and so on. Not the
equivalent of a 
> very big book.
> 
> In order to do this I think the documentation has to be based on html
+ png + 
> svg and so on. This is very different to existing pamphlet files, as
far as I 
> can see the Clojure example you give is not HTML?

Yes, I'm using Latex for most of my work except for Volume 11
which is the new Axiom browser-based help system. I am experimenting
with using <canvas> tags as we speak. See
http://axiom-developer.org/axiom-website/axiomgraph
so I do find HTML5 interesting. 

I have included png and diagrams (see the Clojure pamphlet). 
I have seen 3D animations in PDF files which is a technology
I have bookmarked but not yet tried to use.
> 
> I can't see that it would be very desirable to just put HTML tags into
a mega 
> pamphlet file and sort out the technical issues you mention in your
document. 
> For one thing, one does not want to edit low level HTML using a simple
text 
> editor, I think it really needs WYSIWYG html and graphics editors.
Also the 
> whole idea of hypertext seems to favour a more hierarchical approach
rather 
> than a linear document.

In fact, I do write all of my HTML by hand. I don't use WYSIWYG tools
at all. And the way I use hypertex is the same way I use latex, that 
is to say, I use #name tags into the same document to do things like
jump between sections. This allows the user to jump around in a
linear discussion.

> 
> I want to be able to start with a quick introduction to the program
and when I 
> get to something I'm specially interested to click on a hyperlink and
drill 
> down to more detail. The navigation should give many ways to read the
document 
> including LP-like and Hyperdoc-like and )show-like (computer
generated 
> reference information) all richly interlinked.

The navigation is fine. What I find hard about hyperlinked documents
is that I quickly feel lost. Instead of moving from chapter to chapter
it feels like I'm reading random websites with random browser 
crashes. When I "put the website down" like I would with a book
I don't know how to start again.

You've done a good job at overall organization of your site by
keeping the tree structure at the top. However, when I re-visit
your site I don't know what I've seen and what I haven't.

> 
> We have discussed these things before and I think I'm starting to
repeat 
> myself so, as I say I think it best to agree to disagree. I will watch
your 
> experiment with interest and be ready to be proved wrong.

I don't think you're wrong. I just think we have found our own
approach comfortable. As an old man with a large library I find
reading books to be a useful paradigm. You find the web to be a
useful paradigm. Either approach is fine.

Much more important is the literate programming mindset and
you clearly know that already. Keep up the good work.

\start
Date: Fri, 27 Jan 2012 04:57:31 -0600
From: Tim Daly
To: list
Subject: Axiom January 2012 released

January 2012 Release

The Jenks book was updated by adding appendix E, and the 
graphics images from the center of the book.

These people were added to the credits list:
   Guilherme Reis, Michael Albaugh, Roger House to credits

Literate documentation was added to the book 9, the compiler
The axiomgraphs were created for the compile command and the parser.

The website was updated with several changes including a global
rewrite of the color and fixes for broken links.

Treeshaking of the compiler and interpreter code continues.

Work was done but not completed to move to GCL-2.6.8pre6

bookvol0.pamphlet 
   add Appendix E
   add graphics images from middle of book

bookvol10.4.pamphlet 
   add Guilherme Reis, Michael Albaugh, Roger House

books/bookvol5
   localize function names
   treeshake interpreter
   add Guilherme Reis, Michael Albaugh, Roger House

books/bookvol9 
   add comment graphs
   code cleanup
   localize function names
   treeshake and document compiler

books/bookvolbib 
   add additional Axiom literature references

books/ps/
   add v0hroot.eps, v0hyper.eps, v0page1.eps, v0page2.eps, v0page3.eps
   add v0page4.eps, v0page5.eps, v0page6.eps, v0page7.eps, v0page8.eps
   v9CommentRecording.eps comment recording chapter
   v9CommentSyntaxChecking.eps comment syntax chapter
   v9comdefine.eps document compiler
   v9compiler.eps document compiler

readme
   add Guilherme Reis, Michael Albaugh, Roger House

src/algebra
   Makefile fix autoload

src/axiom-website/
   global background color change to ECEA81
   axiom.png make background transparent
   books.html fix broken links
   litprog.html fix argument count
   litprog.html note HTML escape code flaw

  /axiomgraph/
     axiomgraph.tex literate form of js
     index.html add graphs

  /axiomgraph/js
     arbor.js add graphs
     axiomcode.js add graphs, parse tree, default compiler
     jquery-1.7.1.js add graphs

  /xiomgraph/maps/
     algebra.json add graphs
     comment.json update comment
     add compile.json for the compile command
     add compiler.json for the compiler
     add dstruct.json for data structures
     add funsel.json for function selection
     nine.json for global compiler
     parse.json for parser

  /axiomgraph/style/
     axiomgraph.css change background
     axiomicon.png add icon

  /hyperdoc
     change page background color

src/doc/
   axiom.sty fix comments

src/input/
   Makefile add i2e.input
   i2e.input demo InputForm to Expression(Integer)
   pasta.input graphics test cases

src/interp
   br-con.lisp localize function names, treeshake compiler
   cattable.lisp treeshake compiler
   ht-util.lisp localize function names
   i-funsel.lisp treeshake compiler
   parsing.lisp localize function names, treeshake compiler
   patches.lisp localize function names
   util.lisp treeshake compiler
   vmlisp.lisp localize function names, treeshake compiler

src/scripts/tex/
   axiom.sty fix comments

\start
Date: Mon, 30 Jan 2012 12:40:05 -0500
From: Tim Daly
To: list
Subject: Intrinsic Error Rate

I have been asked to do a video for the Clojure programming 
group about literate programming and this was a recent reply 
explaining the idea. I don't think I've mentioned this special
property of literate programming before but I think it is one
key reason to develop the practice. 

Tim

=================================================================

> > Personally if you have done or would be interested in doing a quick
> > vid cast of how you progress through your workflow, I think that
would
> > be very interesting.
> 
> Sort of "extreme pair programming with everybody"? :-)
> There is no such thing as a simple job but I'll see what I can do.
> A watchable video takes time to compose.

I am composing the notes for this video and I realized I have
a piece of programming information about myself that you probably
do not have about yourself.

Years ago I wrote a program (Archive) to allow users of a time
shared mainframe to back up programs to magnetic tape. This was
a single person project. As an experiment I copied every input
at the keyboard and every output of every command. I also kept
a journal of everything I did, why I did it, and what failed.

The key result was that I discovered what I call my personal
"irreducible error rate". If I do 100 things I will make 3 errors.
This was independent of the task. So typing 100 characters has
3 wrong letters which were mostly caught while typing. Writing
100 lines of code had 3 errors somewhere. Composing email
introduces 3 errors per 100 lines of code.

Most of the errors were trivial (spelling), some were syntax
(missing delimiter), some were semantic (wrong action), some
were design (wrong solution). 

As a result of this measurement I have formed habits to look
for the 3 mistakes I know exist. For example, I am writing a
book of Axiom graphics images (a "gallery"). I have written
679 equations so far, which are going to create the images.

I know there are at least 21 uncaught errors lurking in those
equations. So before I attempt to generate the images I will
examine them in detail with an eye toward finding what I know
is there.

Literate programming is both a source of errors and a help.

It is a source of errors because I have more typing to do
and will generate bad latex and meaningless sentences, as 
well as the "standard" coding errors. So in some sense I have
added to the number of errors I WILL introduce. But the tools,
like the compiler, tex and spellcheck will at least warn me of
these failures.

However, literate programming helps because I am explaining
what I am doing to myself as well as others. This really 
reduces the "big", "costly" errors, those in the design and
implementation. There are no tools to help so I have to create
a discipline that will highlight these errors as soon as
possible.

I KNOW my intrinsic error rate. What is yours? 

What discipline can you apply to your work to find your errors?

I'm not sure how to get this important piece of self-knowledge
into the video so I thought I'm mention it here.

Tim Daly





\end{verbatim}
\eject
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cleardoublepage
%\phantomsection
\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{axiom}
\bibliography{axiom}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cleardoublepage
%\phantomsection
\addcontentsline{toc}{chapter}{Index}
\printindex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
